{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ­ SadTalker: Single Image + Voice from Text (Colab)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OpenTalker/SadTalker/blob/main/colab_single_image_voice_text.ipynb) *(Upload this notebook to Colab if the link points elsewhere.)*\n",
        "\n",
        "**One image, one voice, your text.** The avatar reads your text with lip sync. No voice stripping, no extra variation cost.\n",
        "\n",
        "1. Upload **one face image** (avatar)\n",
        "2. Enter **text** for the avatar to speak\n",
        "3. Pick a **voice** (text-to-speech, free)\n",
        "4. Generate â†’ talking head video with lip sync"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Enable GPU\n",
        "\n",
        "**Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clone SadTalker & install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists('SadTalker'):\n",
        "    !git clone https://github.com/OpenTalker/SadTalker.git\n",
        "%cd SadTalker\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install -q edge-tts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!bash scripts/download_models.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Single image + text â†’ talking video\n",
        "\n",
        "Upload **one** face image, type the text, choose a voice, then run generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import edge_tts\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from pydub import AudioSegment\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from base64 import b64encode\n",
        "\n",
        "BASE_DIR = \"/content/SadTalker\"\n",
        "os.chdir(BASE_DIR)\n",
        "sys.path.insert(0, BASE_DIR)\n",
        "\n",
        "RESULT_DIR = os.path.join(BASE_DIR, \"results\")\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "# Default TTS voices (edge-tts, free)\n",
        "VOICES = [\n",
        "    (\"en-US-JennyNeural (Female)\", \"en-US-JennyNeural\"),\n",
        "    (\"en-US-GuyNeural (Male)\", \"en-US-GuyNeural\"),\n",
        "    (\"en-GB-SoniaNeural (Female, UK)\", \"en-GB-SoniaNeural\"),\n",
        "    (\"en-GB-RyanNeural (Male, UK)\", \"en-GB-RyanNeural\"),\n",
        "]\n",
        "\n",
        "\n",
        "async def text_to_speech_async(text: str, voice: str, out_path: str):\n",
        "    \"\"\"Generate speech from text with edge-tts, save as WAV.\"\"\"\n",
        "    mp3_path = out_path.replace(\".wav\", \".mp3\")\n",
        "    communicate = edge_tts.Communicate(text, voice)\n",
        "    await communicate.save(mp3_path)\n",
        "    # Convert to WAV for SadTalker\n",
        "    audio = AudioSegment.from_mp3(mp3_path)\n",
        "    audio.export(out_path, format=\"wav\")\n",
        "    if os.path.exists(mp3_path):\n",
        "        os.remove(mp3_path)\n",
        "    return out_path\n",
        "\n",
        "\n",
        "def text_to_speech(text: str, voice: str, out_path: str):\n",
        "    return asyncio.run(text_to_speech_async(text, voice, out_path))\n",
        "\n",
        "\n",
        "def run_sadtalker(image_path: str, audio_path: str, result_dir: str) -> str:\n",
        "    \"\"\"Run SadTalker inference. Returns path to generated MP4.\"\"\"\n",
        "    cmd = [\n",
        "        sys.executable, \"inference.py\",\n",
        "        \"--driven_audio\", audio_path,\n",
        "        \"--source_image\", image_path,\n",
        "        \"--result_dir\", result_dir,\n",
        "        \"--still\", \"--preprocess\", \"full\", \"--enhancer\", \"gfpgan\"\n",
        "    ]\n",
        "    subprocess.run(cmd, check=True, cwd=BASE_DIR)\n",
        "    # Newest mp4 in result_dir\n",
        "    mp4s = sorted(Path(result_dir).rglob(\"*.mp4\"), key=os.path.getmtime, reverse=True)\n",
        "    if not mp4s:\n",
        "        raise FileNotFoundError(\"No output video found.\")\n",
        "    return str(mp4s[0])\n",
        "\n",
        "\n",
        "def generate(image_path: str, text: str, voice_name: str):\n",
        "    if not text or not text.strip():\n",
        "        print(\"Please enter some text.\")\n",
        "        return None\n",
        "    if not image_path or not os.path.exists(image_path):\n",
        "        print(\"Please upload a face image.\")\n",
        "        return None\n",
        "    voice_id = dict(VOICES).get(voice_name, VOICES[0][1])\n",
        "    ts = datetime.now().strftime(\"%Y_%m_%d_%H.%M.%S\")\n",
        "    audio_path = os.path.join(RESULT_DIR, f\"tts_{ts}.wav\")\n",
        "    print(\"Generating speech from text...\")\n",
        "    text_to_speech(text.strip(), voice_id, audio_path)\n",
        "    print(\"Running SadTalker (lip sync)...\")\n",
        "    video_path = run_sadtalker(image_path, audio_path, RESULT_DIR)\n",
        "    print(\"Done:\", video_path)\n",
        "    return video_path\n",
        "\n",
        "\n",
        "# UI\n",
        "upload = widgets.FileUpload(accept=\".png,.jpg,.jpeg\", multiple=False, description=\"Face image\")\n",
        "text_in = widgets.Textarea(placeholder=\"Enter the text for the avatar to read...\", rows=4, layout=widgets.Layout(width=\"100%\"))\n",
        "voice_drop = widgets.Dropdown(options=[v[0] for v in VOICES], value=VOICES[0][0], description=\"Voice:\")\n",
        "go_btn = widgets.Button(description=\"Generate video\", button_style=\"primary\")\n",
        "out_area = widgets.Output()\n",
        "\n",
        "def _get_uploaded_file(upload_widget):\n",
        "    \"\"\"Get (filename, bytes) from Colab/ipywidgets FileUpload.\"\"\"\n",
        "    val = upload_widget.value\n",
        "    if not val:\n",
        "        return None, None\n",
        "    if isinstance(val, list) and len(val) > 0:\n",
        "        # Colab sometimes: [{\"name\": \"x.png\", \"content\": b\"...\"}]\n",
        "        item = val[0]\n",
        "        name = item.get(\"name\", \"image.png\")\n",
        "        data = item.get(\"content\", b\"\")\n",
        "        if hasattr(data, \"tobytes\"):\n",
        "            data = data.tobytes()\n",
        "        return name, data\n",
        "    if isinstance(val, dict):\n",
        "        name = list(val.keys())[0]\n",
        "        data = val[name].get(\"content\", val[name]) if isinstance(val[name], dict) else val[name]\n",
        "        if hasattr(data, \"tobytes\"):\n",
        "            data = data.tobytes()\n",
        "        return name, data\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def on_click(btn):\n",
        "    with out_area:\n",
        "        clear_output(wait=True)\n",
        "        name, data = _get_uploaded_file(upload)\n",
        "        if not name or not data:\n",
        "            print(\"Upload a face image first.\")\n",
        "            return\n",
        "        image_path = os.path.join(RESULT_DIR, name)\n",
        "        with open(image_path, \"wb\") as f:\n",
        "            f.write(data)\n",
        "        video_path = generate(image_path, text_in.value, voice_drop.value)\n",
        "        if video_path:\n",
        "            mp4 = open(video_path, \"rb\").read()\n",
        "            data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "            display(HTML(f\"\"\"\n",
        "            <p><b>Output:</b> {os.path.basename(video_path)}</p>\n",
        "            <video width=400 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>\n",
        "            \"\"\"))\n",
        "\n",
        "go_btn.on_click(on_click)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<b>1. Upload one face image (PNG/JPG)</b>\"),\n",
        "    upload,\n",
        "    widgets.HTML(\"<b>2. Text for the avatar to read</b>\"),\n",
        "    text_in,\n",
        "    widgets.HTML(\"<b>3. Voice (TTS)</b>\"),\n",
        "    voice_drop,\n",
        "    go_btn,\n",
        "    widgets.HTML(\"<b>4. Output</b>\"),\n",
        "    out_area,\n",
        "]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
