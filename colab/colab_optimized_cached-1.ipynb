{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ SadTalker: Optimized Cached Setup (Low Cost)\n",
        "\n",
        "**Pre-process face + voice once ‚Üí generate videos from text instantly**\n",
        "\n",
        "**Cost optimization:**\n",
        "- ‚úÖ Pre-process face **once** (saves 3DMM coefficients)\n",
        "- ‚úÖ Pre-process voice **once** (optional - use your voice model)\n",
        "- ‚úÖ Generate videos from **text only** (no face/voice reprocessing)\n",
        "- ‚úÖ **~10x faster** generation (no face detection/3DMM extraction each time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Enable GPU\n",
        "\n",
        "**Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Colab's pre-installed CUDA-enabled PyTorch\n",
        "!pip install -q edge-tts face_alignment imageio imageio-ffmpeg librosa resampy pydub kornia yacs scikit-image basicsr facexlib gfpgan av safetensors gradio\n",
        "!apt-get update -qq && apt-get install -y -qq ffmpeg 2>/dev/null || true\n",
        "\n",
        "import torch\n",
        "print(f\"‚úì PyTorch {torch.__version__}\")\n",
        "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download code + models (same as minimal setup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy Steps 3-4 from colab_minimal_setup.ipynb\n",
        "# This cell should download code and models\n",
        "# For now, assuming they're already downloaded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3.5: Setup Assets Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create assets directory structure\n",
        "import os\n",
        "BASE_DIR = \"/content/SadTalker\"\n",
        "ASSETS_DIR = os.path.join(BASE_DIR, \"assets\")\n",
        "os.makedirs(os.path.join(ASSETS_DIR, \"image\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(ASSETS_DIR, \"audio\"), exist_ok=True)\n",
        "\n",
        "print(\"‚úì Assets directory created\")\n",
        "print(f\"  Image folder: {os.path.join(ASSETS_DIR, 'image')}\")\n",
        "print(f\"  Audio folder: {os.path.join(ASSETS_DIR, 'audio')}\")\n",
        "print(\"\\nüìÅ Upload your files:\")\n",
        "print(\"  - female-image-01.jpg ‚Üí assets/image/\")\n",
        "print(\"  - female-voice-01.mp3 ‚Üí assets/audio/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Optimized Cached Pipeline\n",
        "\n",
        "**Two modes:**\n",
        "1. **Setup Mode**: Upload face image + voice ‚Üí pre-process and cache\n",
        "2. **Generate Mode**: Enter text ‚Üí use cached face/voice ‚Üí fast generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import pickle\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import asyncio\n",
        "import edge_tts\n",
        "from pydub import AudioSegment\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Fix numpy 2.x compatibility\n",
        "if not hasattr(np, 'float'):\n",
        "    np.float = float\n",
        "if not hasattr(np, 'int'):\n",
        "    np.int = int\n",
        "\n",
        "BASE_DIR = \"/content/SadTalker\"\n",
        "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
        "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.chdir(BASE_DIR)\n",
        "sys.path.insert(0, BASE_DIR)\n",
        "\n",
        "RESULT_DIR = os.path.join(BASE_DIR, \"results\")\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "# Cache file paths\n",
        "FACE_CACHE_FILE = os.path.join(CACHE_DIR, \"face_cache.pkl\")\n",
        "VOICE_CACHE_FILE = os.path.join(CACHE_DIR, \"voice_cache.pkl\")\n",
        "\n",
        "# Pre-load your assets (adjust paths as needed)\n",
        "ASSETS_DIR = os.path.join(BASE_DIR, \"assets\")\n",
        "DEFAULT_IMAGE = os.path.join(ASSETS_DIR, \"image\", \"female-image-01.jpg\")\n",
        "DEFAULT_VOICE = os.path.join(ASSETS_DIR, \"audio\", \"female-voice-01.mp3\")\n",
        "\n",
        "\n",
        "def preprocess_and_cache_face(image_path: str, cache_id: str = \"default\"):\n",
        "    \"\"\"Pre-process face once and cache the results.\"\"\"\n",
        "    print(\"Pre-processing face (this runs once)...\")\n",
        "    \n",
        "    from src.utils.preprocess import CropAndExtract\n",
        "    from src.utils.init_path import init_path\n",
        "    import torch\n",
        "    \n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    sadtalker_paths = init_path(CHECKPOINT_DIR, os.path.join(BASE_DIR, 'src/config'), 256, False, 'full')\n",
        "    \n",
        "    preprocess_model = CropAndExtract(sadtalker_paths, device)\n",
        "    \n",
        "    # Extract face coefficients (expensive operation - done once)\n",
        "    cache_frame_dir = os.path.join(CACHE_DIR, f\"face_{cache_id}\")\n",
        "    os.makedirs(cache_frame_dir, exist_ok=True)\n",
        "    \n",
        "    first_coeff_path, crop_pic_path, crop_info = preprocess_model.generate(\n",
        "        image_path, cache_frame_dir, 'full', source_image_flag=True, pic_size=256\n",
        "    )\n",
        "    \n",
        "    if first_coeff_path is None:\n",
        "        return None, \"Face detection failed\"\n",
        "    \n",
        "    # Cache the results\n",
        "    cache_data = {\n",
        "        'first_coeff_path': first_coeff_path,\n",
        "        'crop_pic_path': crop_pic_path,\n",
        "        'crop_info': crop_info,\n",
        "        'image_path': image_path,\n",
        "        'cache_id': cache_id\n",
        "    }\n",
        "    \n",
        "    with open(FACE_CACHE_FILE, 'wb') as f:\n",
        "        pickle.dump(cache_data, f)\n",
        "    \n",
        "    del preprocess_model\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    return cache_data, \"‚úì Face pre-processed and cached!\"\n",
        "\n",
        "\n",
        "def load_face_cache():\n",
        "    \"\"\"Load cached face data.\"\"\"\n",
        "    if os.path.exists(FACE_CACHE_FILE):\n",
        "        with open(FACE_CACHE_FILE, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    return None\n",
        "\n",
        "\n",
        "def preprocess_and_cache_voice(audio_path: str, cache_id: str = \"default\"):\n",
        "    \"\"\"Pre-process voice file (convert to WAV, store path).\"\"\"\n",
        "    print(\"Pre-processing voice file...\")\n",
        "    \n",
        "    # Convert MP3 to WAV if needed\n",
        "    if audio_path.endswith('.mp3'):\n",
        "        wav_path = audio_path.replace('.mp3', '.wav')\n",
        "        if not os.path.exists(wav_path):\n",
        "            audio = AudioSegment.from_mp3(audio_path)\n",
        "            audio.export(wav_path, format=\"wav\")\n",
        "        audio_path = wav_path\n",
        "    \n",
        "    # Cache voice file path\n",
        "    cache_data = {\n",
        "        'voice_path': audio_path,\n",
        "        'cache_id': cache_id\n",
        "    }\n",
        "    \n",
        "    with open(VOICE_CACHE_FILE, 'wb') as f:\n",
        "        pickle.dump(cache_data, f)\n",
        "    \n",
        "    return cache_data, f\"‚úì Voice file cached: {os.path.basename(audio_path)}\"\n",
        "\n",
        "\n",
        "def load_voice_cache():\n",
        "    \"\"\"Load cached voice data.\"\"\"\n",
        "    if os.path.exists(VOICE_CACHE_FILE):\n",
        "        with open(VOICE_CACHE_FILE, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    return None\n",
        "\n",
        "\n",
        "def auto_setup_from_assets():\n",
        "    \"\"\"Automatically setup using default assets if they exist.\"\"\"\n",
        "    face_cache = load_face_cache()\n",
        "    voice_cache = load_voice_cache()\n",
        "    \n",
        "    setup_done = []\n",
        "    \n",
        "    # Setup face if image exists and not cached\n",
        "    if os.path.exists(DEFAULT_IMAGE) and not face_cache:\n",
        "        print(\"Auto-setting up face from assets...\")\n",
        "        cache_data, msg = preprocess_and_cache_face(DEFAULT_IMAGE, \"female-01\")\n",
        "        setup_done.append(f\"Face: {msg}\")\n",
        "    \n",
        "    # Setup voice if audio exists and not cached\n",
        "    if os.path.exists(DEFAULT_VOICE) and not voice_cache:\n",
        "        print(\"Auto-setting up voice from assets...\")\n",
        "        cache_data, msg = preprocess_and_cache_voice(DEFAULT_VOICE, \"female-01\")\n",
        "        setup_done.append(f\"Voice: {msg}\")\n",
        "    \n",
        "    return \"\\n\".join(setup_done) if setup_done else \"‚úì Assets already cached or not found\"\n",
        "\n",
        "\n",
        "async def text_to_speech_async(text: str, voice: str, out_path: str):\n",
        "    \"\"\"Generate speech from text.\"\"\"\n",
        "    mp3_path = out_path.replace(\".wav\", \".mp3\")\n",
        "    communicate = edge_tts.Communicate(text, voice)\n",
        "    await communicate.save(mp3_path)\n",
        "    audio = AudioSegment.from_mp3(mp3_path)\n",
        "    audio.export(out_path, format=\"wav\")\n",
        "    if os.path.exists(mp3_path):\n",
        "        os.remove(mp3_path)\n",
        "    return out_path\n",
        "\n",
        "\n",
        "def generate_video_fast(text: str, use_cached_voice: bool = False):\n",
        "    \"\"\"Fast generation using cached face + TTS or cached voice.\"\"\"\n",
        "    # Load cached face\n",
        "    face_cache = load_face_cache()\n",
        "    if not face_cache:\n",
        "        return None, \"‚ùå No cached face found. Run Setup Mode first.\"\n",
        "    \n",
        "    ts = datetime.now().strftime(\"%Y_%m_%d_%H.%M.%S\")\n",
        "    audio_path = os.path.join(RESULT_DIR, f\"audio_{ts}.wav\")\n",
        "    \n",
        "    # Option 1: Use cached voice file (if available and requested)\n",
        "    if use_cached_voice:\n",
        "        voice_cache = load_voice_cache()\n",
        "        if voice_cache and os.path.exists(voice_cache['voice_path']):\n",
        "            # Use the cached voice file directly\n",
        "            import shutil\n",
        "            shutil.copy(voice_cache['voice_path'], audio_path)\n",
        "            print(f\"Using cached voice: {os.path.basename(voice_cache['voice_path'])}\")\n",
        "        else:\n",
        "            return None, \"‚ùå No cached voice found. Run Setup Mode first or use TTS.\"\n",
        "    else:\n",
        "        # Option 2: Generate speech from text using TTS\n",
        "        VOICES = {\n",
        "            \"en-US-JennyNeural\": \"en-US-JennyNeural\",\n",
        "            \"en-US-GuyNeural\": \"en-US-GuyNeural\",\n",
        "        }\n",
        "        voice_id = \"en-US-JennyNeural\"  # Default female voice\n",
        "        print(\"Generating speech from text...\")\n",
        "        asyncio.run(text_to_speech_async(text.strip(), voice_id, audio_path))\n",
        "    \n",
        "    # Use cached face data for fast inference\n",
        "    print(\"Running fast inference with cached face...\")\n",
        "    \n",
        "    # Create temp dir for this generation\n",
        "    gen_dir = os.path.join(RESULT_DIR, f\"gen_{ts}\")\n",
        "    os.makedirs(gen_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy cached coeff to gen dir\n",
        "    import shutil\n",
        "    cached_coeff = face_cache['first_coeff_path']\n",
        "    gen_coeff_path = os.path.join(gen_dir, os.path.basename(cached_coeff))\n",
        "    shutil.copy(cached_coeff, gen_coeff_path)\n",
        "    \n",
        "    # Run inference with cached face\n",
        "    cmd = [\n",
        "        sys.executable, \"inference.py\",\n",
        "        \"--driven_audio\", audio_path,\n",
        "        \"--source_image\", face_cache['image_path'],\n",
        "        \"--result_dir\", gen_dir,\n",
        "        \"--checkpoint_dir\", CHECKPOINT_DIR,\n",
        "        \"--still\", \"--preprocess\", \"full\", \"--enhancer\", \"gfpgan\"\n",
        "    ]\n",
        "    \n",
        "    env = os.environ.copy()\n",
        "    env[\"PYTHONPATH\"] = BASE_DIR\n",
        "    \n",
        "    r = subprocess.run(cmd, cwd=BASE_DIR, env=env, capture_output=True, text=True)\n",
        "    \n",
        "    if r.returncode != 0:\n",
        "        err = (r.stderr or \"\").strip() or (r.stdout or \"\").strip()\n",
        "        return None, f\"Error: {err}\"\n",
        "    \n",
        "    # Find output video\n",
        "    mp4s = sorted(Path(gen_dir).rglob(\"*.mp4\"), key=os.path.getmtime, reverse=True)\n",
        "    if not mp4s:\n",
        "        return None, \"No output video found\"\n",
        "    \n",
        "    return str(mp4s[0]), f\"‚úì Generated: {os.path.basename(mp4s[0])}\"\n",
        "\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks(title=\"SadTalker ‚Äî Optimized Cached\") as demo:\n",
        "    gr.Markdown(\"## üöÄ Optimized: Pre-process once ‚Üí Generate fast\")\n",
        "    \n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"1Ô∏è‚É£ Setup (Run Once)\"):\n",
        "            gr.Markdown(\"### Pre-process face + voice ‚Üí Cache for fast generation\")\n",
        "            \n",
        "            # Auto-setup from assets\n",
        "            auto_setup_btn = gr.Button(\"üöÄ Auto-Setup from Assets\", variant=\"primary\")\n",
        "            auto_setup_status = gr.Textbox(label=\"Auto-Setup Status\", interactive=False)\n",
        "            \n",
        "            def do_auto_setup():\n",
        "                return auto_setup_from_assets()\n",
        "            \n",
        "            auto_setup_btn.click(fn=do_auto_setup, outputs=[auto_setup_status])\n",
        "            \n",
        "            gr.Markdown(\"---\\n### Or Manual Setup:\")\n",
        "            \n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    setup_image = gr.Image(type=\"filepath\", label=\"Face Image\")\n",
        "                    setup_cache_id = gr.Textbox(label=\"Face Cache ID\", value=\"default\")\n",
        "                    setup_face_btn = gr.Button(\"Pre-process Face\", variant=\"secondary\")\n",
        "                \n",
        "                with gr.Column():\n",
        "                    setup_voice = gr.Audio(type=\"filepath\", label=\"Voice Audio File\")\n",
        "                    setup_voice_id = gr.Textbox(label=\"Voice Cache ID\", value=\"default\")\n",
        "                    setup_voice_btn = gr.Button(\"Cache Voice\", variant=\"secondary\")\n",
        "            \n",
        "            setup_status = gr.Textbox(label=\"Setup Status\", interactive=False)\n",
        "            \n",
        "            def do_setup_face(image, cache_id):\n",
        "                if not image:\n",
        "                    return \"Please upload a face image\"\n",
        "                image_path = image if isinstance(image, str) else image.get(\"path\") or getattr(image, \"name\", None)\n",
        "                cache_data, msg = preprocess_and_cache_face(image_path, cache_id or \"default\")\n",
        "                return msg\n",
        "            \n",
        "            def do_setup_voice(audio, cache_id):\n",
        "                if not audio:\n",
        "                    return \"Please upload a voice audio file\"\n",
        "                audio_path = audio if isinstance(audio, str) else audio.get(\"path\") or getattr(audio, \"name\", None)\n",
        "                cache_data, msg = preprocess_and_cache_voice(audio_path, cache_id or \"default\")\n",
        "                return msg\n",
        "            \n",
        "            setup_face_btn.click(fn=do_setup_face, inputs=[setup_image, setup_cache_id], outputs=[setup_status])\n",
        "            setup_voice_btn.click(fn=do_setup_voice, inputs=[setup_voice, setup_voice_id], outputs=[setup_status])\n",
        "        \n",
        "        with gr.TabItem(\"2Ô∏è‚É£ Generate (Fast)\"):\n",
        "            gr.Markdown(\"### Enter text ‚Üí Generate video (uses cached face + voice)\")\n",
        "            gen_text = gr.Textbox(label=\"Text to speak\", lines=4, placeholder=\"Enter the text for the avatar to read...\")\n",
        "            \n",
        "            with gr.Row():\n",
        "                gen_mode = gr.Radio(\n",
        "                    choices=[\"Use TTS (Text-to-Speech)\", \"Use Cached Voice File\"],\n",
        "                    value=\"Use TTS (Text-to-Speech)\",\n",
        "                    label=\"Audio Source\"\n",
        "                )\n",
        "                gen_btn = gr.Button(\"üöÄ Generate Video\", variant=\"primary\", scale=2)\n",
        "            \n",
        "            gen_video = gr.Video(label=\"Output Video\")\n",
        "            gen_status = gr.Textbox(label=\"Status\", interactive=False, lines=3)\n",
        "            \n",
        "            def do_generate(text, mode):\n",
        "                if not text or not text.strip():\n",
        "                    return None, \"Please enter some text\"\n",
        "                \n",
        "                use_cached = (mode == \"Use Cached Voice File\")\n",
        "                video_path, status = generate_video_fast(text, use_cached_voice=use_cached)\n",
        "                return video_path, status\n",
        "            \n",
        "            gen_btn.click(fn=do_generate, inputs=[gen_text, gen_mode], outputs=[gen_video, gen_status])\n",
        "\n",
        "demo.launch(share=True, server_name=\"0.0.0.0\", server_port=7860)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
