{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ SadTalker: Optimized Cached Setup (Low Cost)\n",
        "\n",
        "**Pre-process face + voice once ‚Üí generate videos from text instantly**\n",
        "\n",
        "**Cost optimization:**\n",
        "- ‚úÖ Pre-process face **once** (saves 3DMM coefficients)\n",
        "- ‚úÖ Pre-process voice **once** (optional - use your voice model)\n",
        "- ‚úÖ Generate videos from **text only** (no face/voice reprocessing)\n",
        "- ‚úÖ **~10x faster** generation (no face detection/3DMM extraction each time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Enable GPU\n",
        "\n",
        "**Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Colab's pre-installed CUDA-enabled PyTorch\n",
        "# Note: face-alignment (hyphen) installs but imports as face_alignment (underscore)\n",
        "!pip install -q edge-tts face-alignment imageio imageio-ffmpeg librosa resampy pydub kornia yacs scikit-image basicsr facexlib gfpgan av safetensors gradio\n",
        "!apt-get update -qq && apt-get install -y -qq ffmpeg 2>/dev/null || true\n",
        "\n",
        "import torch\n",
        "print(f\"‚úì PyTorch {torch.__version__}\")\n",
        "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download code + models (same as minimal setup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download only essential code (src/ + inference.py)\n",
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import shutil\n",
        "\n",
        "BASE_DIR = \"/content/SadTalker\"\n",
        "TEMP_DIR = \"/tmp/sadtalker_extract\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "os.chdir(BASE_DIR)\n",
        "\n",
        "# Download repo as zip\n",
        "repo_url = \"https://github.com/OpenTalker/SadTalker/archive/refs/heads/main.zip\"\n",
        "zip_path = \"/tmp/sadtalker.zip\"\n",
        "print(\"Downloading SadTalker repository...\")\n",
        "urllib.request.urlretrieve(repo_url, zip_path)\n",
        "\n",
        "# Extract entire zip to temp directory\n",
        "print(\"Extracting essential files...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(TEMP_DIR)\n",
        "\n",
        "# Move only src/ and inference.py to BASE_DIR\n",
        "extracted_root = os.path.join(TEMP_DIR, \"SadTalker-main\")\n",
        "if os.path.exists(extracted_root):\n",
        "    # Move src/ directory\n",
        "    src_src = os.path.join(extracted_root, \"src\")\n",
        "    src_dst = os.path.join(BASE_DIR, \"src\")\n",
        "    if os.path.exists(src_src):\n",
        "        if os.path.exists(src_dst):\n",
        "            shutil.rmtree(src_dst)\n",
        "        shutil.move(src_src, src_dst)\n",
        "        print(\"‚úì Moved src/ directory\")\n",
        "    \n",
        "    # Move inference.py\n",
        "    inf_src = os.path.join(extracted_root, \"inference.py\")\n",
        "    inf_dst = os.path.join(BASE_DIR, \"inference.py\")\n",
        "    if os.path.exists(inf_src):\n",
        "        if os.path.exists(inf_dst):\n",
        "            os.remove(inf_dst)\n",
        "        shutil.move(inf_src, inf_dst)\n",
        "        print(\"‚úì Moved inference.py\")\n",
        "\n",
        "# Cleanup\n",
        "shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
        "os.remove(zip_path)\n",
        "\n",
        "# Fix numpy compatibility issue\n",
        "preprocess_file = os.path.join(BASE_DIR, \"src\", \"face3d\", \"util\", \"preprocess.py\")\n",
        "if os.path.exists(preprocess_file):\n",
        "    with open(preprocess_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    fixed = False\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"np.VisibleDeprecationWarning\" in line:\n",
        "            if i > 0 and \"try:\" in lines[i-1]:\n",
        "                continue\n",
        "            \n",
        "            indent = len(line) - len(line.lstrip())\n",
        "            indent_str = \" \" * indent\n",
        "            \n",
        "            lines[i] = f\"{indent_str}try:\\n{indent_str}    warnings.filterwarnings(\\\"ignore\\\", category=np.VisibleDeprecationWarning)\\n{indent_str}except AttributeError:\\n{indent_str}    pass  # VisibleDeprecationWarning removed in newer numpy\\n\"\n",
        "            fixed = True\n",
        "            break\n",
        "    \n",
        "    if fixed:\n",
        "        with open(preprocess_file, \"w\") as f:\n",
        "            f.writelines(lines)\n",
        "        print(\"‚úì Fixed numpy VisibleDeprecationWarning issue\")\n",
        "\n",
        "# Fix np.float in my_awing_arch.py\n",
        "awing_file = os.path.join(BASE_DIR, \"src\", \"face3d\", \"util\", \"my_awing_arch.py\")\n",
        "if os.path.exists(awing_file):\n",
        "    with open(awing_file, \"r\") as f:\n",
        "        content = f.read()\n",
        "    if \"np.float\" in content and \"np.float64\" not in content.split(\"np.float\")[0][-20:]:\n",
        "        import re\n",
        "        content = re.sub(r'\\bnp\\.float\\b', 'np.float64', content)\n",
        "        with open(awing_file, \"w\") as f:\n",
        "            f.write(content)\n",
        "        print(\"‚úì Fixed np.float ‚Üí np.float64 in my_awing_arch.py\")\n",
        "\n",
        "print(f\"\\n‚úì Essential code extracted to {BASE_DIR}\")\n",
        "print(f\"‚úì Found src/ directory: {os.path.exists(os.path.join(BASE_DIR, 'src'))}\")\n",
        "print(f\"‚úì Found inference.py: {os.path.exists(os.path.join(BASE_DIR, 'inference.py'))}\")\n",
        "\n",
        "# Fix face detection: Add face-alignment fallback to croper.py\n",
        "croper_file = os.path.join(BASE_DIR, \"src\", \"utils\", \"croper.py\")\n",
        "if os.path.exists(croper_file):\n",
        "    with open(croper_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    # Check if already patched\n",
        "    content_str = ''.join(lines)\n",
        "    if \"_get_face_alignment\" not in content_str:\n",
        "        # Find insertion point: after imports, before class Preprocesser\n",
        "        insert_idx = None\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"class Preprocesser:\" in line:\n",
        "                insert_idx = i\n",
        "                break\n",
        "        \n",
        "        if insert_idx is not None:\n",
        "            # Insert face-alignment helper functions\n",
        "            patch_code = \"\"\"# Optional: 1adrianb/face-alignment returns 68 points directly (no 98->68 conversion)\n",
        "_FACE_ALIGNMENT = None\n",
        "\n",
        "def _get_face_alignment(device='cuda'):\n",
        "    \\\"\\\"\\\"Lazy-init face_alignment.FaceAlignment (68 landmarks, TWO_D).\\\"\\\"\\\"\n",
        "    global _FACE_ALIGNMENT\n",
        "    if _FACE_ALIGNMENT is None:\n",
        "        try:\n",
        "            import face_alignment\n",
        "            LandmarksType = face_alignment.LandmarksType\n",
        "            # TWO_D or _2D depending on package version\n",
        "            lm_type = getattr(LandmarksType, 'TWO_D', getattr(LandmarksType, '_2D', 1))\n",
        "            fa_device = 'cpu' if device == 'cpu' else 'cuda'\n",
        "            _FACE_ALIGNMENT = face_alignment.FaceAlignment(\n",
        "                lm_type, device=fa_device, face_detector='sfd'\n",
        "            )\n",
        "        except Exception:\n",
        "            _FACE_ALIGNMENT = False\n",
        "    return _FACE_ALIGNMENT if _FACE_ALIGNMENT is not None and _FACE_ALIGNMENT is not False else None\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "            lines.insert(insert_idx, patch_code)\n",
        "            \n",
        "            # Add device to __init__\n",
        "            for i, line in enumerate(lines):\n",
        "                if \"def __init__(self, device='cuda'):\" in line:\n",
        "                    # Find next line with self.predictor\n",
        "                    for j in range(i+1, min(i+5, len(lines))):\n",
        "                        if \"self.predictor = KeypointExtractor\" in lines[j]:\n",
        "                            # Check if device already added\n",
        "                            if \"self.device = device\" not in ''.join(lines[i:j+3]):\n",
        "                                lines.insert(j+1, \"        self.device = device\\n\")\n",
        "                            break\n",
        "                    break\n",
        "            \n",
        "            # Add fallback method before get_landmark\n",
        "            fallback_method = \"\"\"    def _get_landmark_face_alignment(self, img_np, det):\n",
        "        \\\"\\\"\\\"Fallback: use 1adrianb/face-alignment to get 68 landmarks on crop (no 98->68).\\\"\\\"\\\"\n",
        "        fa = _get_face_alignment(self.device)\n",
        "        if fa is None:\n",
        "            return None\n",
        "        try:\n",
        "            img = img_np[int(det[1]):int(det[3]), int(det[0]):int(det[2]), :]\n",
        "            if img.size == 0 or img.shape[0] == 0 or img.shape[1] == 0:\n",
        "                return None\n",
        "            # face_alignment expects RGB; get_landmarks returns list of (68, 2) or (68, 3)\n",
        "            preds = fa.get_landmarks(img)\n",
        "            if not preds or len(preds) == 0:\n",
        "                return None\n",
        "            lm = np.array(preds[0], dtype=np.float64)\n",
        "            if lm.ndim == 2 and lm.shape[0] == 68:\n",
        "                lm = lm[:, :2]\n",
        "            else:\n",
        "                return None\n",
        "            lm[:, 0] += int(det[0])\n",
        "            lm[:, 1] += int(det[1])\n",
        "            return lm\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "\"\"\"\n",
        "            \n",
        "            # Find get_landmark method and insert fallback before it\n",
        "            for i, line in enumerate(lines):\n",
        "                if \"    def get_landmark(self, img_np):\" in line:\n",
        "                    if \"_get_landmark_face_alignment\" not in ''.join(lines[max(0,i-10):i]):\n",
        "                        lines.insert(i, fallback_method)\n",
        "                    break\n",
        "            \n",
        "            # Modify get_landmark to add fallback at the end (before final except)\n",
        "            in_get_landmark = False\n",
        "            for i in range(len(lines)):\n",
        "                if \"    def get_landmark(self, img_np):\" in lines[i]:\n",
        "                    in_get_landmark = True\n",
        "                elif in_get_landmark and lines[i].strip().startswith(\"def \") and \"get_landmark\" not in lines[i]:\n",
        "                    # End of get_landmark method\n",
        "                    break\n",
        "                elif in_get_landmark and \"except Exception:\" in lines[i] and \"# Fallback: face_alignment\" not in ''.join(lines[max(0,i-3):i+1]):\n",
        "                    # Insert fallback before this except\n",
        "                    fallback_call = \"            # Fallback: face_alignment (68 landmarks directly)\\n            lm = self._get_landmark_face_alignment(img_np, det)\\n            return lm\\n\\n\"\n",
        "                    lines.insert(i, fallback_call)\n",
        "                    break\n",
        "            \n",
        "            with open(croper_file, \"w\") as f:\n",
        "                f.writelines(lines)\n",
        "            print(\"‚úì Added face-alignment fallback to croper.py\")\n",
        "        else:\n",
        "            print(\"‚ö† Could not find Preprocesser class in croper.py\")\n",
        "    else:\n",
        "        print(\"‚úì croper.py already patched with face-alignment fallback\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Download models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Download safetensor models\n",
        "print(\"Downloading SadTalker models (safetensor format)...\")\n",
        "models = [\n",
        "    (\"https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/SadTalker_V0.0.2_256.safetensors\", \"SadTalker_V0.0.2_256.safetensors\"),\n",
        "    (\"https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/mapping_00109-model.pth.tar\", \"mapping_00109-model.pth.tar\"),  # For 'full' preprocess\n",
        "]\n",
        "\n",
        "for url, filename in models:\n",
        "    filepath = os.path.join(CHECKPOINT_DIR, filename)\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"  Downloading {filename}...\")\n",
        "        urllib.request.urlretrieve(url, filepath)\n",
        "        print(f\"  ‚úì {filename}\")\n",
        "    else:\n",
        "        print(f\"  ‚úì {filename} (already exists)\")\n",
        "\n",
        "# Download GFPGAN weights for enhancer\n",
        "GFPGAN_DIR = os.path.join(BASE_DIR, \"gfpgan\", \"weights\")\n",
        "os.makedirs(GFPGAN_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\nDownloading GFPGAN enhancer weights...\")\n",
        "gfpgan_models = [\n",
        "    (\"https://github.com/xinntao/facexlib/releases/download/v0.1.0/alignment_WFLW_4HG.pth\", \"alignment_WFLW_4HG.pth\"),\n",
        "    (\"https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_Resnet50_Final.pth\", \"detection_Resnet50_Final.pth\"),\n",
        "    (\"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\", \"GFPGANv1.4.pth\"),\n",
        "    (\"https://github.com/xinntao/facexlib/releases/download/v0.2.2/parsing_parsenet.pth\", \"parsing_parsenet.pth\"),\n",
        "]\n",
        "\n",
        "for url, filename in gfpgan_models:\n",
        "    filepath = os.path.join(GFPGAN_DIR, filename)\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"  Downloading {filename}...\")\n",
        "        urllib.request.urlretrieve(url, filepath)\n",
        "        print(f\"  ‚úì {filename}\")\n",
        "    else:\n",
        "        print(f\"  ‚úì {filename} (already exists)\")\n",
        "\n",
        "print(\"\\n‚úì All models downloaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4.5: Fix torchvision compatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix torchvision compatibility: functional_tensor was moved in newer versions\n",
        "import os\n",
        "import site\n",
        "\n",
        "# Find basicsr installation\n",
        "basicsr_path = None\n",
        "for path in site.getsitepackages():\n",
        "    degradations_file = os.path.join(path, \"basicsr\", \"data\", \"degradations.py\")\n",
        "    if os.path.exists(degradations_file):\n",
        "        basicsr_path = degradations_file\n",
        "        break\n",
        "\n",
        "# Also check common Colab paths\n",
        "if not basicsr_path:\n",
        "    common_paths = [\n",
        "        \"/usr/local/lib/python3.12/dist-packages/basicsr/data/degradations.py\",\n",
        "        \"/usr/local/lib/python3.11/dist-packages/basicsr/data/degradations.py\",\n",
        "        \"/usr/local/lib/python3.10/dist-packages/basicsr/data/degradations.py\",\n",
        "    ]\n",
        "    for p in common_paths:\n",
        "        if os.path.exists(p):\n",
        "            basicsr_path = p\n",
        "            break\n",
        "\n",
        "if basicsr_path:\n",
        "    with open(basicsr_path, \"r\") as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    # Replace the problematic import\n",
        "    old_import = \"from torchvision.transforms.functional_tensor import rgb_to_grayscale\"\n",
        "    new_import = \"from torchvision.transforms.functional import rgb_to_grayscale\"\n",
        "    \n",
        "    if old_import in content and new_import not in content:\n",
        "        content = content.replace(old_import, new_import)\n",
        "        with open(basicsr_path, \"w\") as f:\n",
        "            f.write(content)\n",
        "        print(f\"‚úì Fixed torchvision compatibility in {basicsr_path}\")\n",
        "    elif new_import in content:\n",
        "        print(\"‚úì Torchvision compatibility already fixed\")\n",
        "    else:\n",
        "        print(f\"‚ö† Import line not found in {basicsr_path}\")\n",
        "else:\n",
        "    print(\"‚ö† Could not find basicsr/degradations.py - trying alternative fix...\")\n",
        "    # Alternative: monkey-patch at import time\n",
        "    import sys\n",
        "    \n",
        "    def patch_torchvision():\n",
        "        import torchvision.transforms as transforms_module\n",
        "        if not hasattr(transforms_module, 'functional_tensor'):\n",
        "            from torchvision.transforms import functional as functional_tensor\n",
        "            transforms_module.functional_tensor = functional_tensor\n",
        "    \n",
        "    sys.modules['torchvision.transforms.functional_tensor'] = None\n",
        "    patch_torchvision()\n",
        "    print(\"‚úì Applied runtime patch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3.5: Setup Assets Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create assets directory structure\n",
        "import os\n",
        "BASE_DIR = \"/content/SadTalker\"\n",
        "ASSETS_DIR = os.path.join(BASE_DIR, \"assets\")\n",
        "os.makedirs(os.path.join(ASSETS_DIR, \"image\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(ASSETS_DIR, \"audio\"), exist_ok=True)\n",
        "\n",
        "print(\"‚úì Assets directory created\")\n",
        "print(f\"  Image folder: {os.path.join(ASSETS_DIR, 'image')}\")\n",
        "print(f\"  Audio folder: {os.path.join(ASSETS_DIR, 'audio')}\")\n",
        "print(\"\\nüìÅ Upload your files:\")\n",
        "print(\"  - female-image-01.jpg ‚Üí assets/image/\")\n",
        "print(\"  - female-voice-01.mp3 ‚Üí assets/audio/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Optimized Cached Pipeline\n",
        "\n",
        "**Two modes:**\n",
        "1. **Setup Mode**: Upload face image + voice ‚Üí pre-process and cache\n",
        "2. **Generate Mode**: Enter text ‚Üí use cached face/voice ‚Üí fast generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import pickle\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import asyncio\n",
        "import edge_tts\n",
        "from pydub import AudioSegment\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Fix numpy 2.x compatibility\n",
        "if not hasattr(np, 'float'):\n",
        "    np.float = float\n",
        "if not hasattr(np, 'int'):\n",
        "    np.int = int\n",
        "\n",
        "BASE_DIR = \"/content/SadTalker\"\n",
        "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
        "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.chdir(BASE_DIR)\n",
        "sys.path.insert(0, BASE_DIR)\n",
        "\n",
        "RESULT_DIR = os.path.join(BASE_DIR, \"results\")\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "# Cache file paths\n",
        "FACE_CACHE_FILE = os.path.join(CACHE_DIR, \"face_cache.pkl\")\n",
        "VOICE_CACHE_FILE = os.path.join(CACHE_DIR, \"voice_cache.pkl\")\n",
        "\n",
        "# Pre-load your assets (adjust paths as needed)\n",
        "ASSETS_DIR = os.path.join(BASE_DIR, \"assets\")\n",
        "DEFAULT_IMAGE = os.path.join(ASSETS_DIR, \"image\", \"female-image-01.jpg\")\n",
        "DEFAULT_VOICE = os.path.join(ASSETS_DIR, \"audio\", \"female-voice-01.mp3\")\n",
        "\n",
        "\n",
        "def preprocess_and_cache_face(image_path: str, cache_id: str = \"default\"):\n",
        "    \"\"\"Pre-process face once and cache the results.\"\"\"\n",
        "    try:\n",
        "        print(\"Pre-processing face (this runs once)...\")\n",
        "        \n",
        "        # Validate image path\n",
        "        if not image_path or not os.path.exists(image_path):\n",
        "            return None, f\"‚ùå Image file not found: {image_path}\"\n",
        "        \n",
        "        # Check if image is readable\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            return None, f\"‚ùå Could not read image file: {image_path}. Make sure it's a valid image (PNG/JPG).\"\n",
        "        \n",
        "        from src.utils.preprocess import CropAndExtract\n",
        "        from src.utils.init_path import init_path\n",
        "        import torch\n",
        "        \n",
        "        # Check checkpoints exist\n",
        "        if not os.path.exists(CHECKPOINT_DIR):\n",
        "            return None, f\"‚ùå Checkpoints directory not found: {CHECKPOINT_DIR}\\nPlease run Step 3-4 to download models first.\"\n",
        "        \n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Using device: {device}\")\n",
        "        \n",
        "        try:\n",
        "            sadtalker_paths = init_path(CHECKPOINT_DIR, os.path.join(BASE_DIR, 'src/config'), 256, False, 'full')\n",
        "        except Exception as e:\n",
        "            return None, f\"‚ùå Failed to initialize paths: {str(e)}\\nMake sure checkpoints are downloaded (Step 3-4).\"\n",
        "        \n",
        "        try:\n",
        "            preprocess_model = CropAndExtract(sadtalker_paths, device)\n",
        "        except Exception as e:\n",
        "            return None, f\"‚ùå Failed to load preprocess model: {str(e)}\\nCheck if all model files are downloaded.\"\n",
        "        \n",
        "        # Extract face coefficients (expensive operation - done once)\n",
        "        cache_frame_dir = os.path.join(CACHE_DIR, f\"face_{cache_id}\")\n",
        "        os.makedirs(cache_frame_dir, exist_ok=True)\n",
        "        \n",
        "        try:\n",
        "            first_coeff_path, crop_pic_path, crop_info = preprocess_model.generate(\n",
        "                image_path, cache_frame_dir, 'full', source_image_flag=True, pic_size=256\n",
        "            )\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            if \"face\" in error_msg.lower() or \"detect\" in error_msg.lower():\n",
        "                return None, f\"‚ùå Face detection failed: {error_msg}\\n\\nTip: Use a clear front-facing face image with good lighting.\"\n",
        "            return None, f\"‚ùå Pre-processing error: {error_msg}\"\n",
        "        \n",
        "        if first_coeff_path is None:\n",
        "            return None, \"‚ùå Face detection failed - no face found in image.\\n\\nTip: Use a clear front-facing face image.\"\n",
        "        \n",
        "        # Cache the results\n",
        "        cache_data = {\n",
        "            'first_coeff_path': first_coeff_path,\n",
        "            'crop_pic_path': crop_pic_path,\n",
        "            'crop_info': crop_info,\n",
        "            'image_path': image_path,\n",
        "            'cache_id': cache_id\n",
        "        }\n",
        "        \n",
        "        with open(FACE_CACHE_FILE, 'wb') as f:\n",
        "            pickle.dump(cache_data, f)\n",
        "        \n",
        "        del preprocess_model\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        return cache_data, \"‚úì Face pre-processed and cached!\"\n",
        "    \n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_details = traceback.format_exc()\n",
        "        return None, f\"‚ùå Unexpected error during face pre-processing:\\n{str(e)}\\n\\nDetails:\\n{error_details}\"\n",
        "\n",
        "\n",
        "def load_face_cache():\n",
        "    \"\"\"Load cached face data.\"\"\"\n",
        "    if os.path.exists(FACE_CACHE_FILE):\n",
        "        with open(FACE_CACHE_FILE, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    return None\n",
        "\n",
        "\n",
        "def preprocess_and_cache_voice(audio_path: str, cache_id: str = \"default\"):\n",
        "    \"\"\"Pre-process voice file (convert to WAV, store path).\"\"\"\n",
        "    print(\"Pre-processing voice file...\")\n",
        "    \n",
        "    # Convert MP3 to WAV if needed\n",
        "    if audio_path.endswith('.mp3'):\n",
        "        wav_path = audio_path.replace('.mp3', '.wav')\n",
        "        if not os.path.exists(wav_path):\n",
        "            audio = AudioSegment.from_mp3(audio_path)\n",
        "            audio.export(wav_path, format=\"wav\")\n",
        "        audio_path = wav_path\n",
        "    \n",
        "    # Cache voice file path\n",
        "    cache_data = {\n",
        "        'voice_path': audio_path,\n",
        "        'cache_id': cache_id\n",
        "    }\n",
        "    \n",
        "    with open(VOICE_CACHE_FILE, 'wb') as f:\n",
        "        pickle.dump(cache_data, f)\n",
        "    \n",
        "    return cache_data, f\"‚úì Voice file cached: {os.path.basename(audio_path)}\"\n",
        "\n",
        "\n",
        "def load_voice_cache():\n",
        "    \"\"\"Load cached voice data.\"\"\"\n",
        "    if os.path.exists(VOICE_CACHE_FILE):\n",
        "        with open(VOICE_CACHE_FILE, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    return None\n",
        "\n",
        "\n",
        "def auto_setup_from_assets():\n",
        "    \"\"\"Automatically setup using default assets if they exist.\"\"\"\n",
        "    face_cache = load_face_cache()\n",
        "    voice_cache = load_voice_cache()\n",
        "    \n",
        "    setup_done = []\n",
        "    \n",
        "    # Setup face if image exists and not cached\n",
        "    if os.path.exists(DEFAULT_IMAGE) and not face_cache:\n",
        "        print(\"Auto-setting up face from assets...\")\n",
        "        cache_data, msg = preprocess_and_cache_face(DEFAULT_IMAGE, \"female-01\")\n",
        "        setup_done.append(f\"Face: {msg}\")\n",
        "    \n",
        "    # Setup voice if audio exists and not cached\n",
        "    if os.path.exists(DEFAULT_VOICE) and not voice_cache:\n",
        "        print(\"Auto-setting up voice from assets...\")\n",
        "        cache_data, msg = preprocess_and_cache_voice(DEFAULT_VOICE, \"female-01\")\n",
        "        setup_done.append(f\"Voice: {msg}\")\n",
        "    \n",
        "    return \"\\n\".join(setup_done) if setup_done else \"‚úì Assets already cached or not found\"\n",
        "\n",
        "\n",
        "async def text_to_speech_async(text: str, voice: str, out_path: str):\n",
        "    \"\"\"Generate speech from text.\"\"\"\n",
        "    mp3_path = out_path.replace(\".wav\", \".mp3\")\n",
        "    communicate = edge_tts.Communicate(text, voice)\n",
        "    await communicate.save(mp3_path)\n",
        "    audio = AudioSegment.from_mp3(mp3_path)\n",
        "    audio.export(out_path, format=\"wav\")\n",
        "    if os.path.exists(mp3_path):\n",
        "        os.remove(mp3_path)\n",
        "    return out_path\n",
        "\n",
        "\n",
        "def generate_video_fast(text: str, use_cached_voice: bool = False):\n",
        "    \"\"\"Fast generation using cached face + TTS or cached voice.\"\"\"\n",
        "    # Load cached face\n",
        "    face_cache = load_face_cache()\n",
        "    if not face_cache:\n",
        "        return None, \"‚ùå No cached face found. Run Setup Mode first.\"\n",
        "    \n",
        "    ts = datetime.now().strftime(\"%Y_%m_%d_%H.%M.%S\")\n",
        "    audio_path = os.path.join(RESULT_DIR, f\"audio_{ts}.wav\")\n",
        "    \n",
        "    # Option 1: Use cached voice file (if available and requested)\n",
        "    if use_cached_voice:\n",
        "        voice_cache = load_voice_cache()\n",
        "        if voice_cache and os.path.exists(voice_cache['voice_path']):\n",
        "            # Use the cached voice file directly\n",
        "            import shutil\n",
        "            shutil.copy(voice_cache['voice_path'], audio_path)\n",
        "            print(f\"Using cached voice: {os.path.basename(voice_cache['voice_path'])}\")\n",
        "        else:\n",
        "            return None, \"‚ùå No cached voice found. Run Setup Mode first or use TTS.\"\n",
        "    else:\n",
        "        # Option 2: Generate speech from text using TTS\n",
        "        VOICES = {\n",
        "            \"en-US-JennyNeural\": \"en-US-JennyNeural\",\n",
        "            \"en-US-GuyNeural\": \"en-US-GuyNeural\",\n",
        "        }\n",
        "        voice_id = \"en-US-JennyNeural\"  # Default female voice\n",
        "        print(\"Generating speech from text...\")\n",
        "        asyncio.run(text_to_speech_async(text.strip(), voice_id, audio_path))\n",
        "    \n",
        "    # Use cached face data for fast inference\n",
        "    print(\"Running fast inference with cached face...\")\n",
        "    \n",
        "    # Create temp dir for this generation\n",
        "    gen_dir = os.path.join(RESULT_DIR, f\"gen_{ts}\")\n",
        "    os.makedirs(gen_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy cached coeff to gen dir\n",
        "    import shutil\n",
        "    cached_coeff = face_cache['first_coeff_path']\n",
        "    gen_coeff_path = os.path.join(gen_dir, os.path.basename(cached_coeff))\n",
        "    shutil.copy(cached_coeff, gen_coeff_path)\n",
        "    \n",
        "    # Run inference with cached face\n",
        "    cmd = [\n",
        "        sys.executable, \"inference.py\",\n",
        "        \"--driven_audio\", audio_path,\n",
        "        \"--source_image\", face_cache['image_path'],\n",
        "        \"--result_dir\", gen_dir,\n",
        "        \"--checkpoint_dir\", CHECKPOINT_DIR,\n",
        "        \"--still\", \"--preprocess\", \"full\", \"--enhancer\", \"gfpgan\"\n",
        "    ]\n",
        "    \n",
        "    env = os.environ.copy()\n",
        "    env[\"PYTHONPATH\"] = BASE_DIR\n",
        "    \n",
        "    r = subprocess.run(cmd, cwd=BASE_DIR, env=env, capture_output=True, text=True)\n",
        "    \n",
        "    if r.returncode != 0:\n",
        "        err = (r.stderr or \"\").strip() or (r.stdout or \"\").strip()\n",
        "        return None, f\"Error: {err}\"\n",
        "    \n",
        "    # Find output video\n",
        "    mp4s = sorted(Path(gen_dir).rglob(\"*.mp4\"), key=os.path.getmtime, reverse=True)\n",
        "    if not mp4s:\n",
        "        return None, \"No output video found\"\n",
        "    \n",
        "    return str(mp4s[0]), f\"‚úì Generated: {os.path.basename(mp4s[0])}\"\n",
        "\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks(title=\"SadTalker ‚Äî Optimized Cached\") as demo:\n",
        "    gr.Markdown(\"## üöÄ Optimized: Pre-process once ‚Üí Generate fast\")\n",
        "    \n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"1Ô∏è‚É£ Setup (Run Once)\"):\n",
        "            gr.Markdown(\"### Pre-process face + voice ‚Üí Cache for fast generation\")\n",
        "            \n",
        "            # Auto-setup from assets\n",
        "            auto_setup_btn = gr.Button(\"üöÄ Auto-Setup from Assets\", variant=\"primary\")\n",
        "            auto_setup_status = gr.Textbox(label=\"Auto-Setup Status\", interactive=False)\n",
        "            \n",
        "            def do_auto_setup():\n",
        "                return auto_setup_from_assets()\n",
        "            \n",
        "            auto_setup_btn.click(fn=do_auto_setup, outputs=[auto_setup_status])\n",
        "            \n",
        "            gr.Markdown(\"---\\n### Or Manual Setup:\")\n",
        "            \n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    setup_image = gr.Image(type=\"filepath\", label=\"Face Image\")\n",
        "                    setup_cache_id = gr.Textbox(label=\"Face Cache ID\", value=\"default\")\n",
        "                    setup_face_btn = gr.Button(\"Pre-process Face\", variant=\"secondary\")\n",
        "                \n",
        "                with gr.Column():\n",
        "                    setup_voice = gr.Audio(type=\"filepath\", label=\"Voice Audio File\")\n",
        "                    setup_voice_id = gr.Textbox(label=\"Voice Cache ID\", value=\"default\")\n",
        "                    setup_voice_btn = gr.Button(\"Cache Voice\", variant=\"secondary\")\n",
        "            \n",
        "            setup_status = gr.Textbox(label=\"Setup Status\", interactive=False)\n",
        "            \n",
        "            def do_setup_face(image, cache_id):\n",
        "                try:\n",
        "                    if not image:\n",
        "                        return \"‚ùå Please upload a face image\"\n",
        "                    \n",
        "                    # Handle Gradio file upload format\n",
        "                    if isinstance(image, str):\n",
        "                        image_path = image\n",
        "                    elif hasattr(image, \"name\"):\n",
        "                        image_path = image.name\n",
        "                    elif isinstance(image, dict):\n",
        "                        image_path = image.get(\"path\") or image.get(\"name\")\n",
        "                    else:\n",
        "                        return \"‚ùå Invalid image format. Please upload a PNG or JPG image.\"\n",
        "                    \n",
        "                    if not image_path:\n",
        "                        return \"‚ùå Could not get image path. Please try uploading again.\"\n",
        "                    \n",
        "                    cache_data, msg = preprocess_and_cache_face(image_path, cache_id or \"default\")\n",
        "                    \n",
        "                    if cache_data is None:\n",
        "                        return msg  # Error message\n",
        "                    \n",
        "                    return msg  # Success message\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    import traceback\n",
        "                    return f\"‚ùå Error in setup: {str(e)}\\n\\n{traceback.format_exc()}\"\n",
        "            \n",
        "            def do_setup_voice(audio, cache_id):\n",
        "                if not audio:\n",
        "                    return \"Please upload a voice audio file\"\n",
        "                audio_path = audio if isinstance(audio, str) else audio.get(\"path\") or getattr(audio, \"name\", None)\n",
        "                cache_data, msg = preprocess_and_cache_voice(audio_path, cache_id or \"default\")\n",
        "                return msg\n",
        "            \n",
        "            setup_face_btn.click(fn=do_setup_face, inputs=[setup_image, setup_cache_id], outputs=[setup_status])\n",
        "            setup_voice_btn.click(fn=do_setup_voice, inputs=[setup_voice, setup_voice_id], outputs=[setup_status])\n",
        "        \n",
        "        with gr.TabItem(\"2Ô∏è‚É£ Generate (Fast)\"):\n",
        "            gr.Markdown(\"### Enter text ‚Üí Generate video (uses cached face + voice)\")\n",
        "            gen_text = gr.Textbox(label=\"Text to speak\", lines=4, placeholder=\"Enter the text for the avatar to read...\")\n",
        "            \n",
        "            with gr.Row():\n",
        "                gen_mode = gr.Radio(\n",
        "                    choices=[\"Use TTS (Text-to-Speech)\", \"Use Cached Voice File\"],\n",
        "                    value=\"Use TTS (Text-to-Speech)\",\n",
        "                    label=\"Audio Source\"\n",
        "                )\n",
        "                gen_btn = gr.Button(\"üöÄ Generate Video\", variant=\"primary\", scale=2)\n",
        "            \n",
        "            gen_video = gr.Video(label=\"Output Video\")\n",
        "            gen_status = gr.Textbox(label=\"Status\", interactive=False, lines=3)\n",
        "            \n",
        "            def do_generate(text, mode):\n",
        "                if not text or not text.strip():\n",
        "                    return None, \"Please enter some text\"\n",
        "                \n",
        "                use_cached = (mode == \"Use Cached Voice File\")\n",
        "                video_path, status = generate_video_fast(text, use_cached_voice=use_cached)\n",
        "                return video_path, status\n",
        "            \n",
        "            gen_btn.click(fn=do_generate, inputs=[gen_text, gen_mode], outputs=[gen_video, gen_status])\n",
        "\n",
        "demo.launch(share=True, server_name=\"0.0.0.0\", server_port=7860)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
